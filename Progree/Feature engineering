I am working on the project over Transformer Time Series Model. The data I chose is from previous Kaggle Competition---Optiver, Trading at close. It is a high frequency data for auction book. It consists 198 stocks and their information at each 10 seconds 1 hour before the stock market closes. 
The raw feature are stock_id, date_id, seconds_in_bucket, imbalance size, imbalance_buy_sell_flag, reference_price, matched_size, far_price, near_price, bid_price, bid_size, ask_price, ask_size, wap, time_id, row_id where they are mostly finance terms. Inside of them, far_price and near_price has less than a half of the data, so I am not counting it in the input features. Our target is the 60 second future move in the wap of the stock, less the 60 second future move of the synthetic index.
Previously, I attended this competition but did not have enough time to optimize the training process. Currently, I have built the feature engineering which added new features based on previous ones. I have decided to utilize Transformer Time Series Model and encode on the specific time so that time series will be useful in prediction. Stock_id is a feature inside too so that the model can capture patterns from different Stocks. The reason why I did not separate to train different stocks is because there are too many models to train and the pattern in each models might contribute to the total pattern.
Further steps will be outlier detection--my idea is to detect extreme large and small values than replace them with 90th and 10th percentile (On the raw input, it is weird because I first focused on what I had done before). After this, I will change the target by apply difference or pct_change to that from each stock. The encoding on time will also be carefully implemented. Later on, I will apply Transformer Time series model on a small amount of data to ensure it is working. If not, I will change to another deep learning model.
